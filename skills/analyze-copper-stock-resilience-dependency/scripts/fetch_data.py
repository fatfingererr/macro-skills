#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ•¸æ“šæŠ“å–å·¥å…·

æŠ“å–éŠ…åƒ¹ã€è‚¡å¸‚ä»£ç†ã€ä¸­åœ‹10Yæ®–åˆ©ç‡ç­‰æ•¸æ“šã€‚

æ•¸æ“šä¾†æºï¼š
- éŠ…åƒ¹ï¼šYahoo Finance (HG=F)
- å…¨çƒè‚¡å¸‚å¸‚å€¼ï¼šYahoo Finance (VT)
- ä¸­åœ‹10Yæ®–åˆ©ç‡ï¼šMacroMicro (https://en.macromicro.me/charts/133362/China-10Year-Government-Bond-Yield)
"""

import json
import random
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, List

import pandas as pd
import yfinance as yf

# å¿«å–ç›®éŒ„
CACHE_DIR = Path(__file__).parent.parent / "data" / "cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)

# å–®ä½æ›ç®—ä¿‚æ•¸
POUNDS_PER_TON = 2204.62262

# MacroMicro é…ç½®
MACROMICRO_CHINA_10Y_URL = "https://en.macromicro.me/charts/133362/China-10Year-Government-Bond-Yield"
MACROMICRO_CHART_WAIT_SECONDS = 35
MACROMICRO_MAX_RETRIES = 3

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
]

# Highcharts æ•¸æ“šæå– JavaScript
EXTRACT_HIGHCHARTS_JS = '''
// æª¢æŸ¥ Highcharts æ˜¯å¦å­˜åœ¨
if (typeof Highcharts === 'undefined') {
    return {error: 'Highcharts not loaded', retry: true};
}

// ç²å–æ‰€æœ‰æœ‰æ•ˆçš„åœ–è¡¨
var charts = Highcharts.charts.filter(c => c !== undefined && c !== null);
if (charts.length === 0) {
    return {error: 'No charts found', totalCharts: Highcharts.charts.length, retry: true};
}

// æå–æ¯å€‹åœ–è¡¨çš„æ•¸æ“š
var result = [];
for (var i = 0; i < charts.length; i++) {
    var chart = charts[i];
    var chartInfo = {
        title: chart.title ? chart.title.textStr : 'Chart ' + i,
        series: []
    };

    for (var j = 0; j < chart.series.length; j++) {
        var s = chart.series[j];
        var seriesData = {
            name: s.name,
            type: s.type,
            dataLength: s.data.length,
            data: s.data.map(function(point) {
                return {
                    x: point.x,
                    y: point.y,
                    date: point.x ? new Date(point.x).toISOString().split('T')[0] : null
                };
            })
        };
        chartInfo.series.push(seriesData);
    }
    result.push(chartInfo);
}

return result;
'''


def fetch_copper(
    series: str = "HG=F",
    start_date: str = "2020-01-01",
    end_date: Optional[str] = None,
    freq: str = "1mo",
    convert_to_ton: bool = True,
    use_cache: bool = True,
    cache_hours: int = 12
) -> pd.Series:
    """
    æŠ“å–éŠ…åƒ¹æ•¸æ“š

    Parameters
    ----------
    series : str
        éŠ…åƒ¹åºåˆ—ä»£ç¢¼ï¼ˆé è¨­ HG=Fï¼‰
    start_date : str
        èµ·å§‹æ—¥æœŸ
    end_date : str
        çµæŸæ—¥æœŸï¼ˆé è¨­ä»Šå¤©ï¼‰
    freq : str
        é »ç‡ï¼ˆ1mo, 1wk, 1dï¼‰
    convert_to_ton : bool
        æ˜¯å¦è½‰æ›ç‚º USD/tonï¼ˆé è¨­ Trueï¼‰
    use_cache : bool
        æ˜¯å¦ä½¿ç”¨å¿«å–
    cache_hours : int
        å¿«å–æœ‰æ•ˆæ™‚é–“ï¼ˆå°æ™‚ï¼‰

    Returns
    -------
    pd.Series
        éŠ…åƒ¹æ™‚é–“åºåˆ—
    """
    if end_date is None:
        end_date = datetime.now().strftime("%Y-%m-%d")

    cache_file = CACHE_DIR / f"copper_{series}_{start_date}_{end_date}_{freq}.json"

    # æª¢æŸ¥å¿«å–
    if use_cache and cache_file.exists():
        mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
        if datetime.now() - mtime < timedelta(hours=cache_hours):
            with open(cache_file, "r") as f:
                data = json.load(f)
            series_data = pd.Series(data["values"], index=pd.to_datetime(data["index"]))
            series_data.name = "copper"
            return series_data

    # æŠ“å–æ•¸æ“š
    print(f"æ­£åœ¨æŠ“å–éŠ…åƒ¹æ•¸æ“š: {series}")
    df = yf.download(series, start=start_date, end=end_date, interval=freq, progress=False)

    if df.empty:
        raise ValueError(f"ç„¡æ³•å–å¾— {series} çš„æ•¸æ“š")

    # å–æ”¶ç›¤åƒ¹
    copper = df["Close"].squeeze()

    # å–®ä½æ›ç®—
    if convert_to_ton and series == "HG=F":
        copper = copper * POUNDS_PER_TON
        print(f"å·²å°‡ {series} å¾ USD/lb è½‰æ›ç‚º USD/ton")

    copper.name = "copper"

    # å„²å­˜å¿«å–
    cache_data = {
        "index": copper.index.strftime("%Y-%m-%d").tolist(),
        "values": copper.tolist(),
        "series": series,
        "converted_to_ton": convert_to_ton
    }
    with open(cache_file, "w") as f:
        json.dump(cache_data, f)

    return copper


def fetch_equity(
    series: str = "ACWI",
    start_date: str = "2020-01-01",
    end_date: Optional[str] = None,
    freq: str = "1mo",
    use_cache: bool = True,
    cache_hours: int = 12
) -> pd.Series:
    """
    æŠ“å–è‚¡å¸‚ä»£ç†æ•¸æ“š

    Parameters
    ----------
    series : str
        è‚¡å¸‚åºåˆ—ä»£ç¢¼ï¼ˆé è¨­ ACWIï¼‰
    start_date : str
        èµ·å§‹æ—¥æœŸ
    end_date : str
        çµæŸæ—¥æœŸ
    freq : str
        é »ç‡
    use_cache : bool
        æ˜¯å¦ä½¿ç”¨å¿«å–
    cache_hours : int
        å¿«å–æœ‰æ•ˆæ™‚é–“

    Returns
    -------
    pd.Series
        è‚¡å¸‚æ™‚é–“åºåˆ—
    """
    if end_date is None:
        end_date = datetime.now().strftime("%Y-%m-%d")

    cache_file = CACHE_DIR / f"equity_{series}_{start_date}_{end_date}_{freq}.json"

    # æª¢æŸ¥å¿«å–
    if use_cache and cache_file.exists():
        mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
        if datetime.now() - mtime < timedelta(hours=cache_hours):
            with open(cache_file, "r") as f:
                data = json.load(f)
            series_data = pd.Series(data["values"], index=pd.to_datetime(data["index"]))
            series_data.name = "equity"
            return series_data

    # æŠ“å–æ•¸æ“š
    print(f"æ­£åœ¨æŠ“å–è‚¡å¸‚æ•¸æ“š: {series}")
    df = yf.download(series, start=start_date, end=end_date, interval=freq, progress=False)

    if df.empty:
        raise ValueError(f"ç„¡æ³•å–å¾— {series} çš„æ•¸æ“š")

    equity = df["Close"].squeeze()
    equity.name = "equity"

    # å„²å­˜å¿«å–
    cache_data = {
        "index": equity.index.strftime("%Y-%m-%d").tolist(),
        "values": equity.tolist(),
        "series": series
    }
    with open(cache_file, "w") as f:
        json.dump(cache_data, f)

    return equity


def fetch_world_market_cap(
    start_date: str = "2015-01-01",
    end_date: Optional[str] = None,
    freq: str = "1mo",
    use_cache: bool = True,
    cache_hours: int = 12
) -> pd.Series:
    """
    æŠ“å–å…¨çƒè‚¡å¸‚å¸‚å€¼ä»£ç†æ•¸æ“š

    ä½¿ç”¨ VT (Vanguard Total World Stock ETF) ä½œç‚ºå…¨çƒå¸‚å€¼ä»£ç†

    Parameters
    ----------
    start_date : str
        èµ·å§‹æ—¥æœŸ
    end_date : str
        çµæŸæ—¥æœŸ
    freq : str
        é »ç‡
    use_cache : bool
        æ˜¯å¦ä½¿ç”¨å¿«å–
    cache_hours : int
        å¿«å–æœ‰æ•ˆæ™‚é–“

    Returns
    -------
    pd.Series
        å…¨çƒå¸‚å€¼ä»£ç†åºåˆ—ï¼ˆå–®ä½ï¼šå…†ç¾å…ƒï¼‰
    """
    if end_date is None:
        end_date = datetime.now().strftime("%Y-%m-%d")

    cache_file = CACHE_DIR / f"world_mktcap_{start_date}_{end_date}_{freq}.json"

    # æª¢æŸ¥å¿«å–
    if use_cache and cache_file.exists():
        mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
        if datetime.now() - mtime < timedelta(hours=cache_hours):
            with open(cache_file, "r") as f:
                data = json.load(f)
            series_data = pd.Series(data["values"], index=pd.to_datetime(data["index"]))
            series_data.name = "world_mktcap"
            return series_data

    # æŠ“å– VT æ•¸æ“š
    print(f"æ­£åœ¨æŠ“å–å…¨çƒè‚¡å¸‚å¸‚å€¼ä»£ç†æ•¸æ“š: VT")
    df = yf.download("VT", start=start_date, end=end_date, interval=freq, progress=False)

    if df.empty:
        # å‚™ç”¨ï¼šä½¿ç”¨ ACWI
        print("VT æ•¸æ“šä¸å¯ç”¨ï¼Œæ”¹ç”¨ ACWI")
        df = yf.download("ACWI", start=start_date, end=end_date, interval=freq, progress=False)

    if df.empty:
        raise ValueError("ç„¡æ³•å–å¾—å…¨çƒå¸‚å€¼ä»£ç†æ•¸æ“š")

    price = df["Close"].squeeze()

    # å°‡ ETF åƒ¹æ ¼è½‰æ›ç‚ºä¼°ç®—å¸‚å€¼ï¼ˆå…†ç¾å…ƒï¼‰
    # åŸºæº–ï¼š2024 å¹´å…¨çƒå¸‚å€¼ç´„ 110 å…†ç¾å…ƒï¼Œå°æ‡‰ VT ç´„ 110
    BASE_MKTCAP = 110  # å…†ç¾å…ƒ
    BASE_PRICE = 110   # VT åƒ¹æ ¼åŸºæº–
    world_mktcap = price * (BASE_MKTCAP / BASE_PRICE)
    world_mktcap.name = "world_mktcap"

    # å„²å­˜å¿«å–
    cache_data = {
        "index": world_mktcap.index.strftime("%Y-%m-%d").tolist(),
        "values": world_mktcap.tolist(),
        "source": "VT proxy"
    }
    with open(cache_file, "w") as f:
        json.dump(cache_data, f)

    return world_mktcap


def _fetch_china_yield_macromicro() -> Optional[pd.Series]:
    """
    ä½¿ç”¨ Selenium å¾ MacroMicro æŠ“å–ä¸­åœ‹10Yæ®–åˆ©ç‡æ­·å²æ•¸æ“š

    æ•¸æ“šä¾†æºï¼šhttps://en.macromicro.me/charts/133362/China-10Year-Government-Bond-Yield

    Returns
    -------
    pd.Series or None
        æ®–åˆ©ç‡æ™‚é–“åºåˆ—ï¼Œæˆ– None å¦‚æœå¤±æ•—
    """
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.service import Service
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        from webdriver_manager.chrome import ChromeDriverManager
    except ImportError:
        print("Selenium æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install selenium webdriver-manager")
        return None

    driver = None

    try:
        # 1. éš¨æ©Ÿå»¶é²ï¼ˆæ¨¡æ“¬äººé¡ï¼‰
        delay = random.uniform(1.0, 2.0)
        print(f"è«‹æ±‚å‰å»¶é² {delay:.2f} ç§’...")
        time.sleep(delay)

        # 2. é…ç½® Chrome
        chrome_options = Options()
        chrome_options.add_argument('--headless=new')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument(f'user-agent={random.choice(USER_AGENTS)}')

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(120)

        # 3. è¼‰å…¥é é¢
        print(f"æ­£åœ¨æŠ“å–ä¸­åœ‹10Yæ®–åˆ©ç‡: {MACROMICRO_CHINA_10Y_URL}")
        driver.get(MACROMICRO_CHINA_10Y_URL)

        # 4. åˆæ­¥ç­‰å¾…é é¢è¼‰å…¥
        print("ç­‰å¾…é é¢è¼‰å…¥...")
        time.sleep(5)

        # 5. æ»¾å‹•åˆ°é é¢é ‚éƒ¨
        driver.execute_script('window.scrollTo(0, 0);')
        time.sleep(3)

        # 6. ç­‰å¾…åœ–è¡¨å€åŸŸå‡ºç¾
        print("ç­‰å¾…åœ–è¡¨å€åŸŸ...")
        chart_selectors = [
            '.chart-area',
            '.chart-wrapper',
            '.mm-chart-wrapper',
            '#chartArea',
            '.highcharts-container',
            '[data-highcharts-chart]'
        ]

        for selector in chart_selectors:
            try:
                WebDriverWait(driver, 30).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                print(f"æ‰¾åˆ°åœ–è¡¨å€åŸŸ: {selector}")
                break
            except:
                continue

        # 7. ğŸ”´ é•·æ™‚é–“ç­‰å¾… Highcharts æ¸²æŸ“å®Œæˆ
        print(f"ç­‰å¾…åœ–è¡¨å®Œå…¨æ¸²æŸ“ ({MACROMICRO_CHART_WAIT_SECONDS}ç§’)...")
        time.sleep(MACROMICRO_CHART_WAIT_SECONDS)

        # 8. ç¢ºä¿é é¢ç©©å®š
        driver.execute_script('window.scrollTo(0, 0);')
        time.sleep(2)

        # 9. åŸ·è¡Œ JavaScript æå–æ•¸æ“šï¼ˆå¸¶é‡è©¦ï¼‰
        print("å¾ Highcharts åœ–è¡¨ä¸­æå–æ•¸æ“š...")
        chart_data = None

        for retry in range(MACROMICRO_MAX_RETRIES):
            chart_data = driver.execute_script(EXTRACT_HIGHCHARTS_JS)

            # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡è©¦
            if isinstance(chart_data, dict) and chart_data.get('retry'):
                print(f"é‡è©¦ {retry + 1}/{MACROMICRO_MAX_RETRIES}ï¼Œç­‰å¾… 10 ç§’...")
                time.sleep(10)
                driver.execute_script(
                    'window.scrollTo(0, 100); '
                    'setTimeout(() => window.scrollTo(0, 0), 500);'
                )
                continue
            else:
                break

        # 10. æª¢æŸ¥çµæœ
        if isinstance(chart_data, dict) and 'error' in chart_data:
            print(f"æå–åœ–è¡¨æ•¸æ“šå¤±æ•—: {chart_data['error']}")
            return None

        if not chart_data:
            print("æœªæ‰¾åˆ°åœ–è¡¨æ•¸æ“š")
            return None

        print(f"æˆåŠŸç²å– {len(chart_data)} å€‹åœ–è¡¨çš„æ•¸æ“š!")

        # 11. æ‰¾åˆ°ä¸­åœ‹10Yæ®–åˆ©ç‡ series
        # å¯èƒ½çš„é—œéµå­—ï¼š'China', '10-Year', 'Yield', 'ä¸­åœ‹'
        target_series = None
        for chart in chart_data:
            for series in chart.get('series', []):
                series_name = series.get('name', '')
                # å°‹æ‰¾åŒ…å« China æˆ– Yield æˆ– 10 çš„ series
                if any(kw in series_name.lower() for kw in ['china', 'yield', '10', 'ä¸­åœ‹', 'æ®–åˆ©ç‡']):
                    if series.get('dataLength', 0) > 0:
                        target_series = series
                        print(f"æ‰¾åˆ°ç›®æ¨™ Series: {series_name}, æ•¸æ“šé»: {series['dataLength']}")
                        break
            if target_series:
                break

        # å¦‚æœæ²’æ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹æœ‰æ•¸æ“šçš„ series
        if not target_series:
            for chart in chart_data:
                for series in chart.get('series', []):
                    if series.get('dataLength', 0) > 0:
                        target_series = series
                        print(f"ä½¿ç”¨ç¬¬ä¸€å€‹ Series: {series.get('name')}, æ•¸æ“šé»: {series['dataLength']}")
                        break
                if target_series:
                    break

        if not target_series:
            print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ•¸æ“š series")
            # åˆ—å‡ºæ‰€æœ‰ series ä¾›é™¤éŒ¯
            all_series = [
                f"{s.get('name')} ({s.get('dataLength')} points)"
                for c in chart_data
                for s in c.get('series', [])
            ]
            print(f"å¯ç”¨ series: {all_series}")
            return None

        # 12. è½‰æ›ç‚º Pandas Series
        points = target_series['data']
        df = pd.DataFrame(points)
        df['date'] = pd.to_datetime(df['date'])
        df = df.set_index('date')
        df = df.sort_index()
        df = df.dropna(subset=['y'])

        result = df['y']
        result.name = "cny10y"

        print(f"æˆåŠŸç²å– {len(result)} ç­†ä¸­åœ‹10Yæ®–åˆ©ç‡æ•¸æ“š")
        print(f"  æ—¥æœŸç¯„åœ: {result.index.min()} ~ {result.index.max()}")
        print(f"  æ•¸å€¼ç¯„åœ: {result.min():.2f}% ~ {result.max():.2f}%")

        return result

    except Exception as e:
        print(f"MacroMicro çˆ¬å–å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return None

    finally:
        if driver:
            driver.quit()
            print("ç€è¦½å™¨å·²é—œé–‰")


def fetch_china_10y_yield(
    start_date: str = "2015-01-01",
    end_date: Optional[str] = None,
    use_cache: bool = True,
    cache_hours: int = 12
) -> pd.Series:
    """
    æŠ“å–ä¸­åœ‹10Yæ®–åˆ©ç‡æ•¸æ“š

    æ•¸æ“šä¾†æºï¼šMacroMicro (https://en.macromicro.me/charts/133362/China-10Year-Government-Bond-Yield)

    Parameters
    ----------
    start_date : str
        èµ·å§‹æ—¥æœŸ
    end_date : str
        çµæŸæ—¥æœŸ
    use_cache : bool
        æ˜¯å¦ä½¿ç”¨å¿«å–
    cache_hours : int
        å¿«å–æœ‰æ•ˆæ™‚é–“

    Returns
    -------
    pd.Series
        ä¸­åœ‹10Yæ®–åˆ©ç‡æ™‚é–“åºåˆ—
    """
    if end_date is None:
        end_date = datetime.now().strftime("%Y-%m-%d")

    cache_file = CACHE_DIR / f"china_10y_macromicro.json"

    # æª¢æŸ¥å¿«å–
    if use_cache and cache_file.exists():
        mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
        if datetime.now() - mtime < timedelta(hours=cache_hours):
            print(f"ä½¿ç”¨å¿«å–çš„ä¸­åœ‹10Yæ®–åˆ©ç‡æ•¸æ“š (å¿«å–æ™‚é–“: {mtime.strftime('%Y-%m-%d %H:%M')})")
            with open(cache_file, "r") as f:
                data = json.load(f)
            series_data = pd.Series(data["values"], index=pd.to_datetime(data["index"]))
            series_data.name = "cny10y"

            # éæ¿¾æ—¥æœŸç¯„åœ
            mask = (series_data.index >= start_date) & (series_data.index <= end_date)
            return series_data[mask]

    # å¾ MacroMicro æŠ“å–
    yield_data = _fetch_china_yield_macromicro()

    if yield_data is not None and len(yield_data) > 0:
        # å„²å­˜å¿«å–
        cache_data = {
            "index": yield_data.index.strftime("%Y-%m-%d").tolist(),
            "values": yield_data.tolist(),
            "source": "MacroMicro",
            "url": MACROMICRO_CHINA_10Y_URL,
            "fetched_at": datetime.now().isoformat()
        }
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        print(f"å·²å„²å­˜å¿«å–: {cache_file}")

        # éæ¿¾æ—¥æœŸç¯„åœ
        mask = (yield_data.index >= start_date) & (yield_data.index <= end_date)
        return yield_data[mask]

    # å¦‚æœçˆ¬å–å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨èˆŠçš„å¿«å–ï¼ˆå³ä½¿éæœŸï¼‰
    if cache_file.exists():
        print("çˆ¬å–å¤±æ•—ï¼Œä½¿ç”¨éæœŸçš„å¿«å–æ•¸æ“š")
        with open(cache_file, "r") as f:
            data = json.load(f)
        series_data = pd.Series(data["values"], index=pd.to_datetime(data["index"]))
        series_data.name = "cny10y"
        mask = (series_data.index >= start_date) & (series_data.index <= end_date)
        return series_data[mask]

    # æœ€å¾Œæ‰‹æ®µï¼šæ‹‹å‡ºéŒ¯èª¤
    raise ValueError(
        "ç„¡æ³•å–å¾—ä¸­åœ‹10Yæ®–åˆ©ç‡æ•¸æ“šã€‚\n"
        "è«‹ç¢ºä¿ï¼š\n"
        "1. å·²å®‰è£ selenium: pip install selenium webdriver-manager\n"
        "2. ç¶²è·¯é€£æ¥æ­£å¸¸\n"
        "3. Chrome ç€è¦½å™¨å·²å®‰è£"
    )


def align_monthly(data_dict: Dict[str, pd.Series]) -> pd.DataFrame:
    """
    å°‡å¤šæ¢åºåˆ—å°é½Šåˆ°æœˆåº•

    Parameters
    ----------
    data_dict : dict
        {"copper": series1, "equity": series2, "cny10y": series3}

    Returns
    -------
    pd.DataFrame
        å°é½Šå¾Œçš„ DataFrame
    """
    df = pd.DataFrame(data_dict)

    # ç¢ºä¿ç´¢å¼•ç‚ºæ—¥æœŸ
    df.index = pd.to_datetime(df.index)

    # é‡æ¡æ¨£åˆ°æœˆåº•
    df = df.resample('ME').last()

    # å‰å‘å¡«è£œï¼ˆè™•ç†å°‘é‡ç¼ºå€¼ï¼‰
    df = df.ffill()

    # ä¸Ÿæ‰ä»æœ‰ç¼ºå€¼çš„è¡Œ
    df = df.dropna()

    return df


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="æ•¸æ“šæŠ“å–å·¥å…·")
    parser.add_argument("--series", type=str, required=True, help="åºåˆ—ä»£ç¢¼ï¼ˆé€—è™Ÿåˆ†éš”ï¼‰")
    parser.add_argument("--start", type=str, default="2015-01-01", help="èµ·å§‹æ—¥æœŸ")
    parser.add_argument("--end", type=str, default=None, help="çµæŸæ—¥æœŸ")
    parser.add_argument("--freq", type=str, default="1mo", help="é »ç‡")
    parser.add_argument("--no-cache", action="store_true", help="ä¸ä½¿ç”¨å¿«å–")

    args = parser.parse_args()

    series_list = args.series.split(",")

    for s in series_list:
        s = s.strip()
        if s in ["HG=F"]:
            data = fetch_copper(s, args.start, args.end, args.freq, use_cache=not args.no_cache)
        elif s in ["ACWI", "VT", "URTH"]:
            data = fetch_equity(s, args.start, args.end, args.freq, use_cache=not args.no_cache)
        elif s == "cny10y":
            data = fetch_china_10y_yield(args.start, args.end, use_cache=not args.no_cache)
        else:
            print(f"æœªçŸ¥åºåˆ—: {s}")
            continue

        print(f"\n{s}:")
        print(data.tail())
