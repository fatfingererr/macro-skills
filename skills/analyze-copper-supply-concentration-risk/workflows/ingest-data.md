# Workflow: æ•¸æ“šæ“·å–èˆ‡æ¨™æº–åŒ–

<required_reading>
**è®€å–ä»¥ä¸‹åƒè€ƒæ–‡ä»¶ï¼š**
1. references/data-sources.md
2. thoughts/shared/guide/macromicro-highcharts-crawler.md
</required_reading>

<process>
## Step 1: ç¢ºèªæ“·å–åƒæ•¸

```yaml
commodity: "copper"
start_year: 1970
end_year: 2023
sources:
  - MacroMicro   # å”¯ä¸€ä¸»è¦ä¾†æºï¼ˆWBMS æ•¸æ“šï¼‰
output_dir: "data/"
cache_enabled: true
cache_ttl_days: 7
```

> **æ³¨æ„**ï¼šæœ¬æŠ€èƒ½åƒ…ä½¿ç”¨ MacroMicro (WBMS) ä½œç‚ºç”¢é‡æ•¸æ“šçš„å”¯ä¸€ä¸»è¦ä¾†æºã€‚

## Step 2: æ“·å– MacroMicro éŠ…ç¤¦ç”¢é‡æ•¸æ“š

MacroMicro æä¾› WBMSï¼ˆWorld Bureau of Metal Statisticsï¼‰éŠ…ç¤¦ç”¢é‡æ•¸æ“šï¼ŒåŒ…å«å…¨çƒåŠå„ä¸»è¦ç”¢éŠ…åœ‹çš„æ­·å²ç”¢é‡ã€‚

**æ•¸æ“šæº URL**ï¼š
```
https://en.macromicro.me/charts/91500/wbms-copper-mine-production-total-world
```

**æ“·å–æ–¹å¼**ï¼š
ä½¿ç”¨ Selenium æ¨¡æ“¬ç€è¦½å™¨ï¼Œå¾ Highcharts åœ–è¡¨æå–æ•¸æ“šã€‚

**æ“·å–è…³æœ¬**ï¼š

```python
import random
import time
import json
from pathlib import Path
from datetime import datetime

import pandas as pd

# MacroMicro çˆ¬èŸ²é…ç½®
MACROMICRO_URL = "https://en.macromicro.me/charts/91500/wbms-copper-mine-production-total-world"
CHART_WAIT_SECONDS = 35  # Highcharts æ¸²æŸ“éœ€è¦é•·æ™‚é–“ç­‰å¾…

# User-Agent æ¸…å–®ï¼ˆé˜²åµæ¸¬ï¼‰
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36...',
]

# Highcharts æ•¸æ“šæå– JavaScript
EXTRACT_HIGHCHARTS_JS = '''
(function() {
    if (typeof Highcharts === 'undefined' || !Highcharts.charts) {
        return JSON.stringify({error: 'Highcharts not found', retry: true});
    }

    var charts = Highcharts.charts.filter(c => c !== undefined && c !== null);
    if (charts.length === 0) {
        return JSON.stringify({error: 'No charts found', retry: true});
    }

    var result = [];
    for (var i = 0; i < charts.length; i++) {
        var chart = charts[i];
        var chartInfo = {
            title: chart.title ? chart.title.textStr : 'Chart ' + i,
            series: []
        };

        for (var j = 0; j < chart.series.length; j++) {
            var s = chart.series[j];
            var seriesData = [];

            // å„ªå…ˆä½¿ç”¨ xData/yData
            if (s.xData && s.xData.length > 0) {
                for (var k = 0; k < s.xData.length; k++) {
                    seriesData.push({
                        x: s.xData[k],
                        y: s.yData[k],
                        date: new Date(s.xData[k]).toISOString().split('T')[0]
                    });
                }
            } else if (s.data && s.data.length > 0) {
                seriesData = s.data.map(function(point) {
                    return {
                        x: point.x,
                        y: point.y,
                        date: point.x ? new Date(point.x).toISOString().split('T')[0] : null
                    };
                });
            }

            chartInfo.series.push({
                name: s.name,
                type: s.type,
                dataLength: seriesData.length,
                data: seriesData
            });
        }
        result.push(chartInfo);
    }
    return JSON.stringify(result);
})()
'''


def get_selenium_driver():
    """å»ºç«‹ Selenium WebDriverï¼ˆå¸¶é˜²åµæ¸¬é…ç½®ï¼‰"""
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    from webdriver_manager.chrome import ChromeDriverManager

    chrome_options = Options()
    chrome_options.add_argument('--headless=new')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')

    # é˜²åµæ¸¬è¨­å®š
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
    chrome_options.add_experimental_option('useAutomationExtension', False)

    # éš¨æ©Ÿ User-Agent
    user_agent = random.choice(USER_AGENTS)
    chrome_options.add_argument(f'user-agent={user_agent}')

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.set_page_load_timeout(120)

    return driver


def fetch_macromicro_copper(start_year: int, end_year: int, cache_dir: Path = Path("data/cache")):
    """
    å¾ MacroMicro æ“·å–éŠ…ç”¢é‡æ•¸æ“š

    Returns:
    --------
    pd.DataFrame with columns: year, country, production, unit, source_id, confidence
    """
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC

    cache_dir.mkdir(parents=True, exist_ok=True)
    cache_file = cache_dir / f"macromicro_copper_{start_year}_{end_year}.csv"

    # æª¢æŸ¥å¿«å–
    if cache_file.exists():
        cache_age = datetime.now() - datetime.fromtimestamp(cache_file.stat().st_mtime)
        if cache_age.days < 7:
            print(f"ä½¿ç”¨å¿«å–: {cache_file}")
            return pd.read_csv(cache_file)

    driver = None
    try:
        # éš¨æ©Ÿå»¶é²
        delay = random.uniform(1.0, 2.0)
        print(f"è«‹æ±‚å‰å»¶é² {delay:.2f} ç§’...")
        time.sleep(delay)

        # å•Ÿå‹•ç€è¦½å™¨
        driver = get_selenium_driver()
        print(f"æ­£åœ¨æŠ“å–: {MACROMICRO_URL}")
        driver.get(MACROMICRO_URL)

        # ç­‰å¾…é é¢è¼‰å…¥
        time.sleep(5)
        driver.execute_script('window.scrollTo(0, 0);')
        time.sleep(3)

        # ç­‰å¾…åœ–è¡¨å€åŸŸ
        chart_selectors = ['.highcharts-container', '[data-highcharts-chart]']
        for selector in chart_selectors:
            try:
                WebDriverWait(driver, 30).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                break
            except:
                continue

        # ğŸ”´ é•·æ™‚é–“ç­‰å¾… Highcharts æ¸²æŸ“
        print(f"ç­‰å¾…åœ–è¡¨æ¸²æŸ“ ({CHART_WAIT_SECONDS}ç§’)...")
        time.sleep(CHART_WAIT_SECONDS)

        # åŸ·è¡Œ JavaScript æå–æ•¸æ“š
        result = driver.execute_script(f"return {EXTRACT_HIGHCHARTS_JS}")
        chart_data = json.loads(result) if isinstance(result, str) else result

        if isinstance(chart_data, dict) and 'error' in chart_data:
            raise ValueError(f"æå–å¤±æ•—: {chart_data['error']}")

        # è§£ææ•¸æ“š
        all_data = []
        for chart in chart_data:
            for series in chart.get('series', []):
                series_name = series.get('name', '')
                for point in series.get('data', []):
                    if point.get('y') is None:
                        continue
                    try:
                        year = int(point['date'][:4])
                        if year < start_year or year > end_year:
                            continue
                        all_data.append({
                            'year': year,
                            'country': normalize_country_name(series_name),
                            'production': float(point['y']) * 1000,  # åƒå™¸ -> å™¸
                            'unit': 't_Cu_content',
                            'source_id': 'MacroMicro',
                            'confidence': 0.9,
                            'date': point['date']
                        })
                    except (ValueError, TypeError):
                        continue

        df = pd.DataFrame(all_data)

        # å»é‡ï¼šæ¯å¹´æ¯åœ‹åªä¿ç•™ä¸€ç­†
        df = df.sort_values(['year', 'country', 'date'])
        df = df.groupby(['year', 'country']).last().reset_index()
        df = df[['year', 'country', 'production', 'unit', 'source_id', 'confidence']]

        # ä¿å­˜å¿«å–
        df.to_csv(cache_file, index=False)
        print(f"æ•¸æ“šå·²å¿«å–: {cache_file}")

        return df

    finally:
        if driver:
            driver.quit()
```

## Step 3: åœ‹å®¶åç¨±æ¨™æº–åŒ–

```python
def normalize_country_name(name: str) -> str:
    """æ¨™æº–åŒ–åœ‹å®¶åç¨±"""
    mapping = {
        # è‹±æ–‡è®Šé«”
        "Democratic Republic of the Congo": "Democratic Republic of Congo",
        "DRC": "Democratic Republic of Congo",
        "Congo, Dem. Rep.": "Democratic Republic of Congo",
        "D.R. Congo": "Democratic Republic of Congo",
        "United States of America": "United States",
        "USA": "United States",
        "US": "United States",
        "Russian Federation": "Russia",
        "USSR": "Russia",
        # ä¸­æ–‡
        "æ™ºåˆ©": "Chile",
        "ç§˜é­¯": "Peru",
        "ä¸­åœ‹": "China",
        "ç¾åœ‹": "United States",
        "ä¿„ç¾…æ–¯": "Russia",
        "æ¾³æ´²": "Australia",
        "å¢¨è¥¿å“¥": "Mexico",
        "åŠ æ‹¿å¤§": "Canada",
        "å°å°¼": "Indonesia",
        "è´Šæ¯”äº": "Zambia",
        "å“ˆè–©å…‹": "Kazakhstan",
        "å‰›æœæ°‘ä¸»å…±å’Œåœ‹": "Democratic Republic of Congo",
        "å…¨çƒ": "World",
        "ä¸–ç•Œ": "World",
    }

    if name in mapping:
        return mapping[name]

    name_lower = name.lower()
    for key, value in mapping.items():
        if key.lower() in name_lower or name_lower in key.lower():
            return value

    return name
```

## Step 4: æ•¸æ“šé©—è­‰

```python
def validate_data(df: pd.DataFrame, end_year: int) -> dict:
    """
    é©—è­‰æ•¸æ“šå®Œæ•´æ€§èˆ‡ä¸€è‡´æ€§

    Returns:
    --------
    dict with validation results
    """
    results = {
        "total_records": len(df),
        "year_range": f"{df.year.min()}-{df.year.max()}",
        "countries": df.country.nunique(),
        "has_world_total": "World" in df.country.values,
        "latest_year_records": len(df[df.year == end_year]),
        "issues": []
    }

    # æª¢æŸ¥æ˜¯å¦æœ‰ä¸–ç•Œç¸½é‡
    if not results["has_world_total"]:
        results["issues"].append("ç¼ºå°‘ World ç¸½é‡æ•¸æ“š")

    # æª¢æŸ¥æœ€æ–°å¹´åº¦æ˜¯å¦æœ‰è¶³å¤ åœ‹å®¶
    if results["latest_year_records"] < 10:
        results["issues"].append(f"æœ€æ–°å¹´åº¦ï¼ˆ{end_year}ï¼‰è¨˜éŒ„éå°‘")

    # æª¢æŸ¥ä¸»è¦ç”¢éŠ…åœ‹æ˜¯å¦éƒ½æœ‰æ•¸æ“š
    major_countries = ["Chile", "Peru", "China", "Democratic Republic of Congo"]
    for country in major_countries:
        if country not in df.country.values:
            results["issues"].append(f"ç¼ºå°‘ {country} æ•¸æ“š")

    results["is_valid"] = len(results["issues"]) == 0

    return results
```

## Step 5: ä¿å­˜æ¨™æº–åŒ–æ•¸æ“š

```python
def save_normalized_data(df: pd.DataFrame, output_dir: Path = Path("data")):
    """
    ä¿å­˜æ¨™æº–åŒ–å¾Œçš„æ•¸æ“š
    """
    output_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜å®Œæ•´æ•¸æ“š
    output_file = output_dir / f"copper_production_normalized.csv"
    df.to_csv(output_file, index=False)
    print(f"æ¨™æº–åŒ–æ•¸æ“šå·²ä¿å­˜: {output_file}")

    # ä¿å­˜å…ƒæ•¸æ“š
    metadata = {
        "generated_at": datetime.now().isoformat(),
        "records": len(df),
        "year_range": f"{df.year.min()}-{df.year.max()}",
        "countries": df.country.nunique(),
        "source": "MacroMicro (WBMS)",
        "url": MACROMICRO_URL
    }

    meta_file = output_dir / "copper_production_metadata.json"
    with open(meta_file, "w") as f:
        json.dump(metadata, f, indent=2)

    return output_file
```

## Step 6: å®Œæ•´æ“·å–æµç¨‹

```python
def run_ingestion_pipeline(
    start_year: int = 1970,
    end_year: int = 2023,
    output_dir: Path = Path("data")
):
    """
    åŸ·è¡Œå®Œæ•´æ•¸æ“šæ“·å–æµç¨‹
    """
    print("=" * 50)
    print("éŠ…ç”¢é‡æ•¸æ“šæ“·å–æµç¨‹ï¼ˆæ•¸æ“šä¾†æºï¼šMacroMicroï¼‰")
    print("=" * 50)

    # Step 1: æ“·å– MacroMicro æ•¸æ“š
    print("\n[1/3] æ“·å– MacroMicro æ•¸æ“š...")
    df = fetch_macromicro_copper(start_year, end_year)
    print(f"  - æ“·å– {len(df)} ç­†è¨˜éŒ„")

    # Step 2: é©—è­‰
    print("\n[2/3] é©—è­‰æ•¸æ“š...")
    validation = validate_data(df, end_year)
    print(f"  - é©—è­‰çµæœ: {'é€šé' if validation['is_valid'] else 'æœ‰å•é¡Œ'}")
    if validation["issues"]:
        for issue in validation["issues"]:
            print(f"    âš ï¸ {issue}")

    # Step 3: ä¿å­˜
    print("\n[3/3] ä¿å­˜æ•¸æ“š...")
    output_file = save_normalized_data(df, output_dir)

    print("\n" + "=" * 50)
    print("æ“·å–å®Œæˆï¼")
    print(f"è¼¸å‡ºæª”æ¡ˆ: {output_file}")
    print("=" * 50)

    return df

# åŸ·è¡Œ
if __name__ == "__main__":
    df = run_ingestion_pipeline()
```

## æ›¿ä»£æ–¹æ¡ˆï¼šChrome CDP é€£æ¥

å¦‚æœ Selenium headless è¢« Cloudflare æ“‹ä½ï¼Œå¯ä½¿ç”¨ Chrome CDP æ–¹å¼ï¼š

**Step 1: å•Ÿå‹• Chrome èª¿è©¦æ¨¡å¼**

```bash
# Windows
"C:\Program Files\Google\Chrome\Application\chrome.exe" ^
  --remote-debugging-port=9222 ^
  --remote-allow-origins=* ^
  --user-data-dir="%USERPROFILE%\.chrome-debug-profile" ^
  "https://en.macromicro.me/charts/91500/wbms-copper-mine-production-total-world"
```

**Step 2: ç­‰å¾…é é¢å®Œå…¨è¼‰å…¥**ï¼ˆåœ–è¡¨é¡¯ç¤ºï¼‰

**Step 3: ä½¿ç”¨ CDP æå–æ•¸æ“š**

```python
import requests
import websocket
import json

CDP_PORT = 9222

def get_page_ws_url():
    resp = requests.get(f'http://127.0.0.1:{CDP_PORT}/json', timeout=5)
    pages = resp.json()
    for page in pages:
        if 'macromicro' in page.get('url', '').lower():
            return page.get('webSocketDebuggerUrl')
    return pages[0].get('webSocketDebuggerUrl') if pages else None

def execute_js(ws_url, js_code):
    ws = websocket.create_connection(ws_url, timeout=30)
    cmd = {
        "id": 1,
        "method": "Runtime.evaluate",
        "params": {"expression": js_code, "returnByValue": True}
    }
    ws.send(json.dumps(cmd))
    result = json.loads(ws.recv())
    ws.close()
    return result

# ä½¿ç”¨
ws_url = get_page_ws_url()
result = execute_js(ws_url, EXTRACT_HIGHCHARTS_JS)
chart_data = json.loads(result['result']['result']['value'])
```
</process>

<success_criteria>
æ­¤ workflow å®Œæˆæ™‚ï¼š
- [ ] MacroMicro éŠ…ç”¢é‡æ•¸æ“šå·²æ“·å–
- [ ] æ•¸æ“šå·²æ¨™æº–åŒ–ç‚ºçµ±ä¸€ schema
- [ ] åœ‹å®¶åç¨±å·²æ¨™æº–åŒ–
- [ ] æ•¸æ“šå®Œæ•´æ€§å·²é©—è­‰
- [ ] å¿«å–æ©Ÿåˆ¶æ­£å¸¸é‹ä½œ
- [ ] è¼¸å‡º CSV + å…ƒæ•¸æ“š JSON
</success_criteria>
